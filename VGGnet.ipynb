{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt \n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg(nn.Module):\n",
    "    def __init__(self, drop=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1b = nn.Conv2d(64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.lin1 = nn.Linear(512 * 2 * 2, 4096)\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "\n",
    "        self.lin3 = nn.Linear(4096, 7)\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = F.relu(self.drop(self.lin1(x)))\n",
    "        x = F.relu(self.drop(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "checkpoint = torch.load('VGGNet')\n",
    "net = Vgg().to(device)\n",
    "net.load_state_dict(checkpoint[\"params\"])\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_count(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the top k corrrect count for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, criterion):\n",
    "    net = net.eval()\n",
    "    loss_tr, n_samples = 0.0, 0.0\n",
    "\n",
    "    y_pred = []\n",
    "    y_gt = []\n",
    "\n",
    "    correct_count1 = 0\n",
    "    correct_count2 = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # fuse crops and batchsize\n",
    "        #bs, ncrops, c, h, w = inputs.shape\n",
    "        #bs, c, h, w = inputs.shape\n",
    "        #inputs = inputs.view(-1, c, h, w)\n",
    "        ncrops = 10\n",
    "        bs = 5\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # combine results across the crops\n",
    "        #outputs = outputs.view(bs, ncrops, -1)\n",
    "        #outputs = torch.sum(outputs, dim=1) / ncrops\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # calculate performance metrics\n",
    "        loss_tr += loss.item()\n",
    "\n",
    "        # accuracy\n",
    "        counts = correct_count(outputs, labels, topk=(1, 2))\n",
    "        correct_count1 += counts[0].item()\n",
    "        correct_count2 += counts[1].item()\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        preds = preds.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "        n_samples += labels.size(0)\n",
    "\n",
    "        y_pred.extend(pred.item() for pred in preds)\n",
    "        y_gt.extend(y.item() for y in labels)\n",
    "\n",
    "    acc1 = 100 * correct_count1 / n_samples\n",
    "    acc2 = 100 * correct_count2 / n_samples\n",
    "    loss = loss_tr / n_samples\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Top 1 Accuracy: %2.6f %%\" % acc1)\n",
    "    print(\"Top 2 Accuracy: %2.6f %%\" % acc2)\n",
    "    print(\"Loss: %2.6f\" % loss)\n",
    "    print(\"Precision: %2.6f\" % precision_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"Recall: %2.6f\" % recall_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"F1 Score: %2.6f\" % f1_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_gt, y_pred), '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Get data with no augmentation\n",
    "#trainloader, valloader, testloader = get_dataloaders(augment=False)\n",
    "dataset_path = \"FER2013\"\n",
    "mu, st = 0, 1\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.functional.rgb_to_grayscale,\n",
    "    #transforms.TenCrop(40),\n",
    "    #transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "    #transforms.Lambda(lambda inputs: inputs.view(-1, inputs.shape[1], inputs.shape[2], inputs.shape[3]))\n",
    "])\n",
    "label_corrections = {4: 6, 5:4, 6:5}\n",
    "target_transform = transforms.Lambda(lambda label: label_corrections[label] if label in label_corrections else label)\n",
    "val_dataset = ImageFolder(dataset_path+'/test',transform, target_transform)\n",
    "#plt.imshow(val_dataset[0][0][4].permute(1,2,0))\n",
    "val_loader = DataLoader(dataset=val_dataset,batch_size=5, shuffle=True)\n",
    "print(\"Test\")\n",
    "evaluate(net, val_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
