{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('CLIP-dissect') \n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import similarity\n",
    "import utils\n",
    "import data_utils\n",
    "from clip_dissect_pipeline import dissect_pipeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"FER2013\"\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((48,48))])\n",
    "train_dataset = ImageFolder(dataset_path+'/train',transform)\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=100*6)\n",
    "#creating val data loaders\n",
    "val_dataset = ImageFolder(dataset_path+'/test',transform)\n",
    "val_loader = DataLoader(dataset=val_dataset,batch_size=100)\n",
    "\n",
    "clip_name = 'ViT-B/16'\n",
    "d_probe = 'FER2013'\n",
    "concept_set = 'CLIP-dissect/data/concept_set.txt'\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "similarity_fn = similarity.soft_wpmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "model_path = 'face_emotion_deep_emotion.pth'\n",
    "target_model = models.Deep_Emotion().to('cuda')\n",
    "target_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, neuron_image_indices = dissect_pipeline(d_probe,concept_set, similarity_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import utils\n",
    "import sys\n",
    "import pytorchcv\n",
    "import data_utils1\n",
    "\n",
    "import cbm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dir = \"Label-free-CBM/saved_model_jason_concept/saved_models/cifar100_lf_cbm\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = cbm.load_cbm(load_dir, device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Update the dataset transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224, 224), interpolation=transforms.InterpolationMode.BILINEAR), # Adjusting input size\n",
    "    # Add normalization here if required, e.g., transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_dataset = ImageFolder(root='FER2013/test', transform=transform)\n",
    "\n",
    "# Update the model's final layer to match the number of classes in your dataset\n",
    "num_classes = len(val_dataset.classes)\n",
    "model.proj_layer = torch.nn.Linear(in_features=7178, out_features=num_classes, bias=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.final = torch.nn.Linear(in_features=7, out_features=num_classes, bias=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(dataset_path+\"/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_set_path = 'CLIP-dissect/data/concept_set.txt'\n",
    "with open(concept_set_path, 'r') as file:  \n",
    "    concepts = file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_g = torch.load(os.path.join(load_dir, \"W_g.pt\"), map_location=device)\n",
    "print(\"Before adjustment:\", W_g)  # Check if it's all zeros\n",
    "\n",
    "# Manually adjust a weight for debugging (this is just an example, adjust according to your actual dimensions)\n",
    "W_g[0][0] = 1.0\n",
    "\n",
    "# Load into the model\n",
    "model.final.weight.data = W_g\n",
    "print(\"After adjustment:\", model.final.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = random.choices([i for i in range(len(classes))], k=1)\n",
    "\n",
    "for i in to_show:\n",
    "    print(\"Output class:{} - {}\".format(i, classes[i]))\n",
    "    print(\"Incoming weights:\")\n",
    "    for j in range(len(concepts)):\n",
    "        #if torch.abs(model.final.weight[i,j])>0.05:\n",
    "            print(\"{} [{:.4f}] {}\".format(concepts[j], model.final.weight[i,j], classes[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
