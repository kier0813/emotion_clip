{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('CLIP-dissect') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'jaro_winkler' from 'jellyfish' (c:\\Users\\kier0\\anaconda3\\envs\\facebase\\lib\\site-packages\\jellyfish\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msimilarity\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_utils\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kier0\\anaconda3\\envs\\facebase\\lib\\site-packages\\similarity\\__init__.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_score_and_distance_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_distance_function_to_score_function\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_score_and_distance_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_weighted_similarity_function\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWord\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word\n",
      "File \u001b[1;32mc:\\Users\\kier0\\anaconda3\\envs\\facebase\\lib\\site-packages\\similarity\\convert_score_and_distance_functions.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meditdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m get_edit_distance\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjellyfish\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jaro_winkler \u001b[38;5;28;01mas\u001b[39;00m get_similarity\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_distance_function_to_score_function\u001b[39m(function\u001b[38;5;241m=\u001b[39mget_edit_distance):\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_func\u001b[39m(s1, s2):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'jaro_winkler' from 'jellyfish' (c:\\Users\\kier0\\anaconda3\\envs\\facebase\\lib\\site-packages\\jellyfish\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import similarity\n",
    "import utils\n",
    "import data_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"FER2013\"\n",
    "class_names = os.listdir(dataset_path+\"/train\")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = []\n",
    "for file_name in glob.glob(dataset_path+'/train/*/*'):\n",
    "    emotion = file_name.split('/')[-2]\n",
    "    if emotion not in emotions:\n",
    "        img = cv2.imread(file_name)\n",
    "    emotions.append(emotion)\n",
    "    \n",
    "emotions = []\n",
    "for file_name in glob.glob(dataset_path+'/test/*/*'):\n",
    "    emotion = file_name.split('/')[-2]\n",
    "    if emotion not in emotions:\n",
    "        img = cv2.imread(file_name)\n",
    "    emotions.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((48,48))])\n",
    "\n",
    "train_dataset = ImageFolder(dataset_path+'/train',transform)\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=100*6)\n",
    "#creating val data loaders\n",
    "val_dataset = ImageFolder(dataset_path+'/test',transform)\n",
    "val_loader = DataLoader(dataset=val_dataset,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change this part\n",
    "d_probe = train_dataset\n",
    "concept_set = class_names\n",
    "similarity_fn = similarity.soft_wpmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissect_pipeline(d_probe, concept_set, similarity_fn):\n",
    "    '''\n",
    "    Function that fixes setting for CLIP-dissect\n",
    "    d_probe: images that are being used\n",
    "    concept_set: concept set that are being used to use CLIP-dissect\n",
    "    similarity_fn: kind of function to be used to measure \n",
    "    '''\n",
    "    clip_name = 'ViT-B/16'\n",
    "    d_probe = d_probe\n",
    "    concept_set = concept_set\n",
    "    batch_size = 50\n",
    "    device = 'cuda'\n",
    "    pool_mode = 'avg'\n",
    "\n",
    "    save_dir = 'saved_activations'\n",
    "    similarity_fn = similarity_fn\n",
    "\n",
    "    target_name = 'emotion'\n",
    "    target_layer = 'fc2' # last layer\n",
    "\n",
    "    utils.save_activations(clip_name = clip_name, target_name = target_name, target_layers = [target_layer], \n",
    "                       d_probe = d_probe, concept_set = concept_set, batch_size = batch_size, \n",
    "                       device = device, pool_mode=pool_mode, save_dir = save_dir)\n",
    "\n",
    "    with open(concept_set, 'r') as f: \n",
    "        words = (f.read()).split('\\n')\n",
    "\n",
    "    pil_data = data_utils.get_data(d_probe)\n",
    "    save_names = utils.get_save_names(clip_name = clip_name, target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  concept_set = concept_set, pool_mode=pool_mode,\n",
    "                                  save_dir = save_dir)\n",
    "    target_save_name, clip_save_name, text_save_name = save_names\n",
    "\n",
    "    similarities, target_feats = utils.get_similarity_from_activations(target_save_name, clip_save_name, \n",
    "                                                                  text_save_name, similarity_fn, device=device)\n",
    "    # Visualize\n",
    "    top_vals, top_ids = torch.topk(target_feats, k=5, dim=0)\n",
    "    neurons_to_check = torch.sort(torch.max(similarities, dim=1)[0], descending=True)[1][0:20]\n",
    "    font_size = 14\n",
    "    font = {'size'   : font_size}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    predict_lst = []\n",
    "    neuron_image_indices = {}\n",
    "    neuron_image_paths = {}\n",
    "    fig = plt.figure(figsize=[10, len(neurons_to_check)*2])\n",
    "    subfigs = fig.subfigures(nrows=len(neurons_to_check), ncols=1)\n",
    "\n",
    "    for j, orig_id in enumerate(neurons_to_check):\n",
    "        vals, ids = torch.topk(similarities[orig_id], k=5, largest=True)\n",
    "        neuron_top_ids = top_ids[:, orig_id].cpu().numpy() \n",
    "\n",
    "        # Get the file indices for top images, ensuring indices are within range\n",
    "        valid_top_indices = [idx for idx in neuron_top_ids if idx < len(pil_data.samples)]\n",
    "        invalid_top_indices = [idx for idx in neuron_top_ids if idx >= len(pil_data.samples)]\n",
    "        neuron_image_indices[orig_id] = valid_top_indices\n",
    "        neuron_image_paths[orig_id] = [pil_data.samples[idx][0] for idx in valid_top_indices]\n",
    "\n",
    "        subfig = subfigs[j]\n",
    "        subfig.text(0.13, 0.96, \"Neuron {}:\".format(int(orig_id)), size=font_size)\n",
    "        subfig.text(0.27, 0.96, \"CLIP-Dissect:\", size=font_size)\n",
    "        subfig.text(0.4, 0.96, words[int(ids[0])], size=font_size)\n",
    "        predict_lst.append(words[int(ids[0])])\n",
    "        axs = subfig.subplots(nrows=1, ncols=5)\n",
    "        for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "            im, label = pil_data[top_id]\n",
    "            im = im.resize([375,375])\n",
    "            axs[i].imshow(im)\n",
    "            axs[i].axis('off')\n",
    "    plt.show()\n",
    "    return predict_lst, neuron_image_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "model_path = 'face_emotion_deep_emotion.pth'\n",
    "target_model = models.Deep_Emotion().to('cuda')\n",
    "target_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_probe = 'FER2013'\n",
    "concept_set = 'CLIP-dissect/data/emotions.txt' # concept set needs to be path to .txt file\n",
    "similarity_fn = similarity.soft_wpmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change dataset path and dataset in data_utils.py in order to run your wanted CLIP-dissect with image\n",
    "dissect_pipeline(d_probe,concept_set, similarity_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_actual_labels(image_indices, pil_data):\n",
    "    labels = []\n",
    "    for idx in image_indices:\n",
    "        path, _ = pil_data.samples[idx]\n",
    "        label = path.split('/')[-2]  \n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_results = {}\n",
    "\n",
    "for neuron, predicted_emotion in zip(neuron_image_indices.keys(), predicted_emotions):\n",
    "    neuron_id = neuron.item() \n",
    "    image_indices = neuron_image_indices[neuron]\n",
    "    actual_labels = get_actual_labels(image_indices, pil_data)\n",
    "    \n",
    "    detailed_results[neuron_id] = {\n",
    "        'predicted_emotion': predicted_emotion,\n",
    "        'actual_labels': actual_labels,\n",
    "        'top_image_indices': image_indices,\n",
    "    }\n",
    "\n",
    "    correct_count = actual_labels.count(predicted_emotion)\n",
    "    \n",
    "    # Calculate the accuracy for this neuron\n",
    "    accuracy = correct_count / len(image_indices)\n",
    "    detailed_results[neuron_id]['accuracy'] = accuracy\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = sum(d['accuracy'] for d in detailed_results.values()) / len(detailed_results)\n",
    "\n",
    "# Print the detailed results\n",
    "for neuron_id, results in detailed_results.items():\n",
    "    print(f\"Neuron {neuron_id}: Predicted Emotion = {results['predicted_emotion']}, Accuracy = {results['accuracy']}\")\n",
    "    for image_idx, actual_label in zip(results['top_image_indices'], results['actual_labels']):\n",
    "        print(f\"    Image Index {image_idx}: Actual Label = {actual_label}\")\n",
    "print(f\"Overall accuracy: {overall_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
